{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import io\n",
    "import imageio\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import widgets, Layout, HBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, n_filters):\n",
    "    x = layers.Conv2D(n_filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Conv2D(n_filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_samp(x, n_filters):\n",
    "    x = conv_block(x, n_filters)\n",
    "    skip = layers.MaxPooling2D(padding='same')(x)\n",
    "    return x, skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_samp(x, skip, n_filters):\n",
    "    x = layers.Conv2DTranspose(n_filters, 2, 2, padding='same')(x)\n",
    "    x = layers.Concatenate()([x, skip])\n",
    "    x = conv_block(x, n_filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(input_shape, out_c, n_filters=8):\n",
    "    x = keras.Input(input_shape)\n",
    "    \n",
    "    # downsampling\n",
    "    d1, p1 = down_samp(x, n_filters)\n",
    "    d2, p2 = down_samp(p1, n_filters*2)\n",
    "    d3, p3 = down_samp(p2, n_filters*4)\n",
    "    d4, p4 = down_samp(p3, n_filters*8)\n",
    "    \n",
    "    # bottleneck\n",
    "    b = conv_block(p4, n_filters*16)\n",
    "    \n",
    "    # upsampling\n",
    "    u1 = up_samp(b, d4, n_filters*8)\n",
    "    u2 = up_samp(u1, d3, n_filters*4)\n",
    "    u3 = up_samp(u2, d2, n_filters*2)\n",
    "    u4 = up_samp(u3, d1, n_filters)\n",
    "    \n",
    "    y = layers.Conv2D(out_c, (1, 1), padding='same')(u4)\n",
    "    \n",
    "    model = keras.Model(x, y, name='u-net')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shapes: (460, 63, 64, 64, 4), (460, 63, 64, 64, 3)\n",
      "Validation Dataset Shapes: (52, 63, 64, 64, 4), (52, 63, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "fpath = 'data/sim_np/size64/sim_512x64x64x64x3.npy'\n",
    "dataset = np.load(fpath)\n",
    "fpath = 'data/sim_np/size64/bound_64x64.npy'\n",
    "boundary = np.load(fpath)\n",
    "\n",
    "# Swap the axes representing the number of frames and number of data samples.\n",
    "# dataset = np.swapaxes(dataset, 0, 1)\n",
    "# We'll pick out 1000 of the 10000 total examples and use those.\n",
    "# dataset = dataset[:1000, ...]\n",
    "# Add a channel dimension since the images are grayscale.\n",
    "# dataset = np.expand_dims(dataset, axis=-1)\n",
    "\n",
    "# # Split into train and validation sets using indexing to optimize memory.\n",
    "indexes = np.arange(dataset.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_index = indexes[: int(0.9 * dataset.shape[0])]\n",
    "val_index = indexes[int(0.9 * dataset.shape[0]) :]\n",
    "train_dataset = dataset[train_index]\n",
    "val_dataset = dataset[val_index]\n",
    "\n",
    "# Normalize the data to the 0-1 range.\n",
    "# train_dataset = train_dataset / 255\n",
    "# val_dataset = val_dataset / 255\n",
    "\n",
    "# We'll define a helper function to shift the frames, where\n",
    "# `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n",
    "def create_shifted_frames(data, boundary):\n",
    "    x = np.zeros((data.shape[0], data.shape[1] - 1, data.shape[2], data.shape[3], data.shape[4] + 1), np.float16)\n",
    "    y = np.zeros((data.shape[0], data.shape[1] - 1, data.shape[2], data.shape[3], data.shape[4]), np.float16)\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1] - 1):\n",
    "            \n",
    "            x[i, j] = np.concatenate((data[i, j], np.expand_dims(boundary, axis=-1)), axis=-1)\n",
    "            y[i, j] = data[i, j + 1]\n",
    "        \n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Apply the processing function to the datasets.\n",
    "x_train, y_train = create_shifted_frames(train_dataset, boundary)\n",
    "x_val, y_val = create_shifted_frames(val_dataset, boundary)\n",
    "\n",
    "# Inspect the dataset.\n",
    "print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
    "print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_frames(x):\n",
    "    return x.reshape(x.shape[0]*x.shape[1], x.shape[2], x.shape[3], x.shape[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data.\n",
    "X = unroll_frames(x_train)\n",
    "Y = unroll_frames(y_train)\n",
    "X_val = unroll_frames(x_val)\n",
    "Y_val = unroll_frames(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 16:45:47.070830: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-14 16:45:47.074036: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 16:46:13.825807: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-14 16:46:16.316281: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - ETA: 0s - loss: 1.0030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 16:46:42.765476: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 30s 116ms/step - loss: 1.0030 - val_loss: 0.8419 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 29s 126ms/step - loss: 0.5215 - val_loss: 0.5421 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 23s 101ms/step - loss: 0.4474 - val_loss: 0.4538 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 22s 98ms/step - loss: 0.4068 - val_loss: 0.4209 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 29s 126ms/step - loss: 0.3790 - val_loss: 0.3900 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 29s 127ms/step - loss: 0.3629 - val_loss: 0.3679 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 14s 59ms/step - loss: 0.3502 - val_loss: 0.3639 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 13s 57ms/step - loss: 0.3373 - val_loss: 0.3373 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 13s 57ms/step - loss: 0.3280 - val_loss: 0.3242 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 13s 57ms/step - loss: 0.3187 - val_loss: 0.3144 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Define modifiable training hyperparameters.\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Define some callbacks to improve training.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=0)\n",
    "\n",
    "# create model to train\n",
    "model = UNet((64, 64, 4), 3, 8)\n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "\n",
    "# fit data to model\n",
    "history = model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data = (X_val, Y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 16:49:51.477446: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/keras/unet_mseloss/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('models/keras/unet_mseloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.002987027168274,\n",
       "  0.5214625000953674,\n",
       "  0.4473508596420288,\n",
       "  0.40680235624313354,\n",
       "  0.3790105879306793,\n",
       "  0.3629036247730255,\n",
       "  0.3502279818058014,\n",
       "  0.33727866411209106,\n",
       "  0.32802167534828186,\n",
       "  0.3186872899532318],\n",
       " 'val_loss': [0.8418557643890381,\n",
       "  0.5421290993690491,\n",
       "  0.4538145959377289,\n",
       "  0.42091190814971924,\n",
       "  0.38995546102523804,\n",
       "  0.36787837743759155,\n",
       "  0.3639078140258789,\n",
       "  0.3372558057308197,\n",
       "  0.32422807812690735,\n",
       "  0.31440332531929016],\n",
       " 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0606d5e76c7e1327171f83de1beba49393fd7daa96a8e17a806b31b3ab97db7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tf-39-cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
